================================================================================
CHUNK 4 OF 5
================================================================================

========================================
File: ./examples/eg_1.py
========================================
from webpath import WebPath
from collections import Counter
from datetime import datetime, timedelta

print("=" * 80)
print("---- BEFORE: Using requests + manual everything ----")
print("=" * 80)

before_code = '''import requests
from urllib.parse import urlencode, urljoin
from collections import Counter
from datetime import datetime, timedelta
import time

def analyze_trending_languages_old():
    base_url = "https://api.github.com"
    
    # step 1: build the  search URL with query parameters
    one_week_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
    params = {
        'q': f'created:>{one_week_ago}',
        'sort': 'stars',
        'order': 'desc',
        'per_page': 10
    }
    
    search_url = urljoin(base_url, '/search/repositories')
    query_string = urlencode(params)
    full_url = f"{search_url}?{query_string}"
    
    try:
        # step2: get the trending repos
        resp = requests.get(full_url, headers={'Accept': 'application/vnd.github.v3+json'})
        resp.raise_for_status()
        search_results = resp.json()
        
        language_counts = Counter()
        
        # step3: loop through the results and get 5 
        for repo in search_results.get('items', [])[:5]:
            if repo.get('language'):
                language_counts[repo['language']] += 2
            
            # Step 4: Get contributors URL and fetch
            contributors_url = repo.get('contributors_url')
            if not contributors_url:
                continue
                
            try:
                contrib_resp = requests.get(contributors_url, params={'per_page': 5})
                contrib_resp.raise_for_status()
                contributors = contrib_resp.json()
                
                # last step another loop
                for contributor in contributors[:3]:
                    user_url = contributor.get('url')
                    if not user_url:
                        continue
                    
                    try:
                        user_resp = requests.get(user_url)
                        user_resp.raise_for_status()
                        user_data = user_resp.json()
                        
                        repos_url = user_data.get('repos_url')
                        if not repos_url:
                            continue
                        
                        repos_resp = requests.get(repos_url, params={'per_page': 10, 'sort': 'stars'})
                        repos_resp.raise_for_status()
                        user_repos = repos_resp.json()
                        
                        for repo in user_repos[:5]:
                            if repo.get('language'):
                                language_counts[repo['language']] += 1
                                
                    except requests.RequestException:
                        continue
                        
            except requests.RequestException:
                continue
            
            time.sleep(0.1)  # Be nice to GitHub's API
        
        return language_counts.most_common(10)
        
    except requests.RequestException as e:
        print(f"Error: {e}")
        return []

results = analyze_trending_languages_old()
'''

print(before_code)

print("\n" + "=" * 80)
print("AFTER: Using WebPath (24 lines)")
print("=" * 80)

after_code = '''from webpath import WebPath
from collections import Counter
from datetime import datetime, timedelta

def analyze_trending_languages_new():
    api = WebPath("https://api.github.com")
    
    one_week_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
    trending = (api / "search" / "repositories").with_query(
        q=f'created:>{one_week_ago}',
        sort='stars',
        order='desc',
        per_page=10
    ).get()
    
    language_counts = Counter()
    
    # nav through results naturally
    for i, repo in enumerate(trending['items'][:5]):
        if repo.get('language'):
            language_counts[repo['language']] += 2
        
        contributors = trending / 'items' / i / 'contributors_url'
        for j in range(min(3, len(contributors.json_data))):
            user = contributors / j / 'url'
            user_repos = user / 'repos_url'
            
            for repo in user_repos.json_data[:5]:
                if repo.get('language'):
                    language_counts[repo['language']] += 1
    
    return language_counts.most_common(10)

results = analyze_trending_languages_new()
'''

print(after_code)

print("\n" + "=" * 80)
print("RUNNING THE WEBPATH VERSION")
print("=" * 80)

####################actual wp code to fetch trending languages

api = WebPath("https://api.github.com")

print("\nFetching this week's trending repositories...")
one_week_ago = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
trending = (api / "search" / "repositories").with_query(
    q=f'created:>{one_week_ago} stars:>100',  
    sort='stars',
    order='desc', 
    per_page=20
).get()

languages = Counter()
for repo in trending['items']:
    if repo.get('language'):
        languages[repo['language']] += 1

print(f"\nLanguages in {len(trending['items'])} trending repos:")
for lang, count in languages.most_common(8):
    bar = "*" * (count * 2)
    print(f"{lang:12} {bar} {count}")
========================================
File: ./webpath/cache.py
========================================
from __future__ import annotations

from typing import Optional
import hashlib
import json
import time
from pathlib import Path

class CacheConfig:
    def __init__(self, ttl: int = 300, cache_dir: Optional[Path] = None):
        self.ttl = ttl
        self.cache_dir = cache_dir or Path.home() / ".webpath" / "cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
    
    def _cache_key(self, verb: str, url: str) -> str:
        key_str = f"{verb.upper()}:{url}"
        return hashlib.md5(key_str.encode()).hexdigest()
    
    def _cache_path(self, verb: str, url: str) -> Path:
        key = self._cache_key(verb, url)
        return self.cache_dir / f"{key}.json"
    
    def get(self, verb: str, url: str) -> Optional[dict]:
        cache_path = self._cache_path(verb, url)
        if not cache_path.exists():
            return None
        
        try:
            with cache_path.open('r') as f:
                cached = json.load(f)
            
            if time.time() - cached['timestamp'] > self.ttl:
                cache_path.unlink(missing_ok=True)
                return None
            
            return cached
        except (json.JSONDecodeError, KeyError, OSError):
            cache_path.unlink(missing_ok=True)
            return None
    
    def set(self, verb: str, url: str, response) -> None:
        cache_path = self._cache_path(verb, url)
        
        sensitive_headers = {
            'authorization', 'cookie', 'x-api-key', 'x-auth-token', 
            'authentication', 'proxy-authorization'
        }
        safe_headers = {
            k: v for k, v in response.headers.items() 
            if k.lower() not in sensitive_headers
        }
        
        cached = {
            'timestamp': time.time(),
            'status_code': response.status_code,
            'headers': safe_headers,
            'content': response.content.decode('utf-8', errors='ignore'),
            'url': response.url
        }
        
        try:
            with cache_path.open('w') as f:
                json.dump(cached, f)
        except OSError:
            pass
========================================
File: ./webpath/downloads.py
========================================
from __future__ import annotations

import hashlib
import importlib
import os
from pathlib import Path
from typing import Optional

from webpath._http import http_request

def download_file(
    url,
    dest: str | os.PathLike,
    *,
    chunk: int = 8192,
    progress: bool = True,
    retries: int = 3,
    backoff: float = 0.3,
    checksum: Optional[str] = None,
    algorithm: str = "sha256",
    **req_kw,
):
    dest = Path(dest)
    bar = None
    
    try:
        r = http_request("get", url, stream=True, retries=retries, backoff=backoff, **req_kw)
        r.raise_for_status()

        total = int(r.headers.get("content-length", 0))
        hasher = hashlib.new(algorithm) if checksum else None

        if progress:
            try:
                mod = importlib.import_module("tqdm")
                if hasattr(mod, "tqdm"):
                    bar = mod.tqdm(total=total, unit="B", unit_scale=True, leave=False)
            except ModuleNotFoundError:
                pass

        with dest.open("wb") as fh:
            for block in r.iter_content(chunk):
                if block:
                    fh.write(block)
                    if hasher:
                        hasher.update(block)
                    if bar:
                        bar.update(len(block))
                        
    except Exception:
        if dest.exists():
            dest.unlink(missing_ok=True)
        raise
    finally:
        if bar:
            bar.close()

    if checksum and hasher and hasher.hexdigest() != checksum.lower():
        dest.unlink(missing_ok=True)
        raise ValueError(
            f"Checksum mismatch for {dest.name}: "
            f"expected {checksum}, got {hasher.hexdigest()}"
        )
    return dest
========================================
File: ./examples/eg_3_json_test.py
========================================
from webpath import WebPath

def test_jsonplaceholder_users():
    print("Testing JSON Shortcuts with JSONPlaceholder API (Users)\n")
    
    api = WebPath("https://jsonplaceholder.typicode.com").with_logging()
    
    response = (api / "users").get()
    
    print("Using .find() for nested user data:")
    
    for i in range(3):
        name = response.find(f"{i}.name")
        email = response.find(f"{i}.email")
        city = response.find(f"{i}.address.city")
        company = response.find(f"{i}.company.name")
        website = response.find(f"{i}.website")
        
        print(f"User {i+1}: {name}")
        print(f"Email: {email}")
        print(f"City: {city}")
        print(f"Company: {company}")
        print(f"Website: {website}")
        print()

def test_jsonplaceholder_posts():
    api = WebPath("https://jsonplaceholder.typicode.com").with_logging()
    
    response = (api / "posts").get()
    
    print(f"\nFound {len(response.json_data)} posts")
    
    all_titles = response.find_all("*.title")
    print("\nFirst 5 post titles:")
    for i, title in enumerate(all_titles[:5]):
        print(f"{i+1}. {title}")

def test_single_user():
    api = WebPath("https://jsonplaceholder.typicode.com").with_logging()
    
    response = (api / "users" / "1").get()
    
    lat = response.find("address.geo.lat")
    lng = response.find("address.geo.lng")
    
    print(f"Name: {response.find('name')}")
    print(f"Coordinates: {lat}, {lng}")

if __name__ == "__main__":
    test_jsonplaceholder_users()
    test_jsonplaceholder_posts() 
    test_single_user()
========================================
File: ./tests/test_query_params.py
========================================
from webpath import WebPath

def test_query_with_lists():
    url = WebPath("https://api.com").with_query(tags=["python", "web"], limit=10)
    assert "tags=python" in str(url)
    assert "tags=web" in str(url)
    assert "limit=10" in str(url)

def test_query_with_none_removes_param():
    url = WebPath("https://api.com?existing=value&remove=old")
    updated = url.with_query(remove=None, new="added")
    
    assert "remove=" not in str(updated)
    assert "existing=value" in str(updated)
    assert "new=added" in str(updated)

def test_query_preserves_existing():
    """with_query should preserve existing parameters"""
    url = WebPath("https://api.com?keep=this&modify=old")
    updated = url.with_query(modify="new", add="more")
    
    assert "keep=this" in str(updated)
    assert "modify=new" in str(updated)
    assert "add=more" in str(updated)
    assert "modify=old" not in str(updated)

def test_query_with_tuples():
    url = WebPath("https://api.com").with_query(coords=(1.23, 4.56))
    assert "coords=1.23" in str(url)
    assert "coords=4.56" in str(url)
========================================
File: ./tests/test_validation.py
========================================

from webpath import WebPath
import pytest

def test_webpath_requires_scheme():
    with pytest.raises(ValueError, match="must include scheme"):
        WebPath("example.com/path")

def test_webpath_rejects_non_http_schemes():
    with pytest.raises(ValueError, match="Only http/https schemes supported"):
        WebPath("ftp://example.com/file")
    
    with pytest.raises(ValueError, match="Only http/https schemes supported"):
        WebPath("file:///etc/passwd")

def test_webpath_requires_hostname():
    with pytest.raises(ValueError, match="must include hostname"):
        WebPath("https:///path/only")

def test_webpath_empty_url():
    with pytest.raises(ValueError, match="cannot be empty"):
        WebPath("")
    
    with pytest.raises(ValueError, match="cannot be empty"):
        WebPath("   ")
